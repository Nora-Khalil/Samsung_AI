{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e32a2d-707b-404c-a692-ca38f8c1072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "res = rq.get('https://en.wikipedia.org/wiki/Machine_learning')\n",
    "res.status_code\n",
    "# res.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8b22c-be76-440b-a3a1-0b0dad8476cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4\n",
    "res = rq.get('https://en.wikipedia.org/wiki/Machine_learning')\n",
    "soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "x=soup.find_all('p')\n",
    "text=''\n",
    "for i in range(len(x)):\n",
    "    text += x[i].text.strip() + '\\n'\n",
    "    # print(text)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2a21e3-a696-45e0-a0ed-ae1f96ac666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "my_regex = re.compile('([0-9]+)[^0-9]+([0-9]+)')\n",
    "m = my_regex.search(\"Anna is 15 years old and John is 12 years old.\")\n",
    "print(m.group(0))\n",
    "print(m.group(1))\n",
    "print(m.group(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb26687-32a6-427e-905d-736e28766e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_regex = re.compile('(\\D+)((\\d+)\\D+(\\d+)\\D+(\\d+))')\n",
    "m = my_regex.search(\"John 010-1234-5678\")\n",
    "print(\"Phone number : \" + m.group(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfaa19e-5eb5-404f-a7ec-045f65000fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#require to do this first\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a5017-d187-4097-a5b3-ae526a4aee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "s = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
    "print(word_tokenize(s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ab8d2-1b49-4f49-9fd2-a32156bf3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27c7037-21cf-4f11-9d6b-0d94a3c56796",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(WordPunctTokenizer().tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1fab02-3cf3-4178-8835-98f350980374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence \n",
    "print(text_to_word_sequence(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717b315-29f5-4ee1-bfaf-750286587f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d19523e-73fb-4942-b1a6-c5d7d9f00644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2ab79-064d-4323-b0d0-f38f3f6af516",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(example) \n",
    "result=[] \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        result. append(w) \n",
    "print (word_tokens) \n",
    "print(result) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b8e00b-a2ba-45db-8243-36646ba0b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('all')\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11838715-97f0-4cbd-b44a-ed560685db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "n=WordNetLemmatizer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dies', 'watched', 'ha', 'starting']\n",
    "print([n.lemmatize(w) for w in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffadc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.lemmatize('dies','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c86a5e-0c05-4315-af65-cc08c9bd30cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.lemmatize('watched','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e3454-4f06-4c3f-830a-3421a607539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.lemmatize('has','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc60215-d5b8-492b-b94d-4c25f90e0903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "s = PorterStemmer() \n",
    "text=\" This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "words=word_tokenize(text)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de792c6a-3905-4509-9e3c-cdf204ae777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883ef4a-1a06-442d-9171-b1d4c78fc7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['formalize', 'allowance', 'electricical']\n",
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95646efc-7666-4894-82f3-8960d6cf33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "s=PorterStemmer()\n",
    "words=['policy' , 'doing' , 'organization' , 'have', 'going' , 'lives', 'fly', 'love', 'dies', 'watched', 'has', 'starting'] \n",
    "print([s.stem(w) for w in words]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34380c9b-a2ad-41fd-b4b2-459cc4414728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer \n",
    "l=LancasterStemmer()\n",
    "words=['policy' , 'doing' , 'organization' , 'have', 'going' , 'lives', 'fly', 'love', 'dies', 'watched', 'has', 'starting']\n",
    "print([l.stem(w) for w in words]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a49d15-a75d-4fff-ab6a-f1e246f3382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence=\"The Colosseum was built by the emperor Vespassian\" \n",
    "import nltk\n",
    "my_words = nltk.word_tokenize(my_sentence) \n",
    "print(my_words)\n",
    "for i in range(len(my_words)): \n",
    "     my_words[i] = my_words[i].lower() \n",
    "my_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b342d-1f86-443f-be4a-9377d81f0228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk. tokenize import word_tokenize \n",
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5ac02-0cf2-4956-be91-1cfd85ad5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_words_tagged = nltk.pos_tag(my_words) \n",
    "my_words_tagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3518acbf-3ae7-4547-930a-7934f787a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b18e4-6326-42d8-831d-6813ba0a5b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk. tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92df76-0a61-4b6a-95d2-786e4a7b059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
    "text = sent_tokenize(text)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6456b-a57d-447e-ae7f-03af881a5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in text:\n",
    "    sentence = word_tokenize(i)\n",
    "    result = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        word = word.lower() \n",
    "        if word not in stop_words: \n",
    "            if len(word) > 2: \n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0\n",
    "                vocab[word] += 1\n",
    "    sentences.append(result)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7487a4-d126-4da8-b9e8-9e29b6254cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf1cd47-539f-4491-ab60-7ac6e3946279",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab['barber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4eace3-5926-438e-9f73-909443a08e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467f9a4-5c44-441a-9b92-48918dbe74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key = lambda x: x[1], reverse=True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cff255-bdd5-4af3-9f29-5c8d3398db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab_sorted:\n",
    "    if frequency > 1:\n",
    "        i = i + 1\n",
    "        word_to_index[word] = i\n",
    "        \n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2198c-230f-4f7f-9b21-4336fca101a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5\n",
    "\n",
    "word_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1]\n",
    "\n",
    "for w in word_frequency:\n",
    "    del word_to_index[w]\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb000e94-4ea7-4799-98c9-768d0908ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index['OOV'] = len(word_to_index) + 1\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0219db0-534d-4aa8-93bc-f978d0ac1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = []\n",
    "for s in sentences:\n",
    "    temp = []\n",
    "    for w in s:\n",
    "        try: \n",
    "            temp.append(word_to_index[w])\n",
    "        except KeyError:\n",
    "            \n",
    "            temp.append(word_to_index['OOV'])\n",
    "    encoded.append(temp)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5328867-bf4e-4a1c-a652-ad901d8a0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "words = sum(sentences, [])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f2bc0-638b-4051-a327-08839a6a3a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ab3ab-ca00-4010-9cae-a07b939dc8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab['barber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d562010-8a02-4a0d-8999-7992b5bd0fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7784d9c3-f4b5-4d11-a10d-39d5a5fd3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word,frequency) in vocab:\n",
    "    i = i+1\n",
    "    word_to_index[word] = i\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e72aa-9c5e-4c79-81cc-b2a0e61847f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "vocab = FreqDist(np.hstack(sentences))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f283d4b-8542-421e-b65b-ce98c8657968",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab['barber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c667a590-bcb3-49b5-8bad-5d0968f5e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c76a08-bce2-4bbe-aef5-383222a8650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e3899-3cc6-4725-8156-a7abc1f9d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = ['a','b','c','d','e']\n",
    "for index, value in enumerate(test_input):\n",
    "    print('value : {} , index : {}'.format(value, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128eafe-45cd-49cd-9046-aaff235e29ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5db7ba6-f24e-403b-b624-6e2f896d93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3c170-5633-4e2d-90bf-39faf87bfbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a034a-edea-4771-a6fa-377b39852d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681aeaa0-0c1d-47a8-a355-366a23ee34bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ac431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
